# Machine Learning (Summer 2023)
This is a repository to keep track of my Machine Learning journey in the Summer of 2023. As part of this journey, I have been joining competitions on Kaggle, primarily those that are related to Natural Language Processing, as I find textual data most intriguing. Below is a short summary of the competitions I partook and a note of how good/bad I did in those competitions and a short blurb about what I've learned in that competition: 

## [Not Competition] Amazon Review Classifier
This is not part of a competition, but a comon NLP task that is used for text classifier tutorials on the Internet. Here, I used the "Prime Pantry 5" data set found in the following [link](https://cseweb.ucsd.edu/~jmcauley/datasets/amazon/links.html). The objective of the classifier is to predict the star-rating of pantry products from text reviews on Amazon. This was the very first time I used *tensorflow-hub* to build clasifiers. 

## [Kaggle Competition] Contradiction Classifier (My Dear Watson)
This is part of a Kaggle competition called "[Contradictory, My Dear Watson](https://www.kaggle.com/competitions/contradictory-my-dear-watson)". The objective of the classifier is to predict whether the premise and hypothesis are entailments, contradictions, or neither. This was my very first attempt to build a classifier that takes two textual features as input(s). This was the second time I used *transformers*. 

- Best Performance: 7/60 with 87.41% test accuracy (Aug 7th, 2023)

## [Kaggle Competition] Disaster Tweets Classifier (NLP with Disaster Tweets)
This is part of a Kaggle competition called "[Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started)". The objective of the classifier is to use tweets to predict whether the tweet is about an actual disaster or not (binary classification task). This was my very first time I used *transformers*. 

- Best Performance: 93/1002 with 83.97% test accuracy (July 26th, 2023)

## [Kaggle Competition] Grammar Classifier
Although this is part of a Kaggle competition called "[CoLA In-Domain Open Evaluation](https://www.kaggle.com/competitions/cola-in-domain-open-evaluation/leaderboard)", I did not make submissions to the leaderboard. The objective of the classifier is to predict whether the sentence is grammatically correct or not. This was my second attempt at using *tensorflow-hub*. 

- Best Performance: ??

## [Not Competition] Sarcasm Classifier
This is not part of a competition but uses a very popular dataset known as the [News Headlines Dataset for Sarcasm Detection](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection). The objective of the classifier is to predict whether a news headline is sarcastic or not. This was my third attempt at using *tensorflow-hub* and the first time I added a bi-directional LSTM layer on top of the BERT-based classifier(s) that I had been working on previously. 

## [Not Competition] Tensorflow Lectures

## [Not Competition] Text Generation Model








