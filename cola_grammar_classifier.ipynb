{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19mt_ILxNf5OW96NPj2iU2S5qicxLbfpu",
      "authorship_tag": "ABX9TyOw2hkjyt1X8n/5HWbCQCr5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lee-messi/machine-learning/blob/main/cola_grammar_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow --quiet\n",
        "!pip install tensorflow-hub --quiet\n",
        "!pip install tensorflow-text --quiet\n",
        "!pip install tensorflow-addons --quiet\n",
        "!pip install tensorflow-datasets --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5lnTCkSPEbt",
        "outputId": "908fa9d7-4ec7-47cc-b4cf-35a71eddbef6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.7 requires jax>=0.4.6, but you have jax 0.3.25 which is incompatible.\n",
            "flax 0.6.11 requires jax>=0.4.2, but you have jax 0.3.25 which is incompatible.\n",
            "orbax-checkpoint 0.2.6 requires jax>=0.4.9, but you have jax 0.3.25 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\""
      ],
      "metadata": {
        "id": "T_5ey_StOQgd"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.environ['COLAB_TPU_ADDR']:\n",
        "  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "  tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "  print('Using TPU')\n",
        "elif tf.config.list_physical_devices('GPU'):\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "  print('Using GPU')\n",
        "else:\n",
        "  raise ValueError('Running on CPU is not recommended.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-KR1d2fP0ff",
        "outputId": "ff36eeb5-3922-4490-d8aa-c6c145c5bdbd"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import CoLA (The Corpus of Linguistic Acceptability) Data"
      ],
      "metadata": {
        "id": "Da8AGPhK2yzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CoLA is a collection of sentences that has been annotated for acceptability. The sentences are coded/labeled 1 if the sentence is grammatically acceptable or correct, and they are coded 0 if the sentence is grammatically wrong. You can access the publicly available corpus using the link [here](https://nyu-mll.github.io/CoLA/). First, we are going to import the datasets. The datasets are provided in **.tsv** format which can be imported using the **read_csv()** function."
      ],
      "metadata": {
        "id": "8kpW_bAY25mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('drive/MyDrive/Colab Notebooks/glue-tasks/in_domain_train.tsv',\n",
        "                    sep = '\\t', header = None)\n",
        "val = pd.read_csv('drive/MyDrive/Colab Notebooks/glue-tasks/in_domain_dev.tsv',\n",
        "                  sep = '\\t', header = None)\n",
        "test = pd.read_csv('drive/MyDrive/Colab Notebooks/glue-tasks/out_of_domain_dev.tsv',\n",
        "                   sep = '\\t', header = None)"
      ],
      "metadata": {
        "id": "A7EtWaOqXdC0"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns = ['misc', 'labels', 'na', 'text']\n",
        "val.columns = ['misc', 'labels', 'na', 'text']\n",
        "test.columns = ['misc', 'labels', 'na', 'text']"
      ],
      "metadata": {
        "id": "A47VxiErc66F"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'{}{}{}'.format(train.shape, val.shape, test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "hMbIOAoGZ_4j",
        "outputId": "795ba3e8-ca37-480c-90c3-5ef7ed1086e1"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(8551, 4)(527, 4)(516, 4)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we are going to inspect the training dataset to ensure that it is balanced - the number of sentences that are grammatically correct and wrong are equal. This is so that the classifier is not biased."
      ],
      "metadata": {
        "id": "INKwyclWGXjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.labels.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MlOFWP7GlNq",
        "outputId": "939840e7-e8ec-4f1f-b125-b60f579c97f4"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    6023\n",
              "0    2528\n",
              "Name: labels, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 6,023 sentences with label 1 and 2,528 sentences with label 0. Let's balance these out."
      ],
      "metadata": {
        "id": "r_8ADa9MGqN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.groupby('labels').sample(1000)"
      ],
      "metadata": {
        "id": "9JA_a_NKx46r"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we are going to implement the **df_to_dataset()** function to create a **tf.data.Dataset** using the balanced reviews dataset. This allows us to map the features in the pandas dataframe to features that are more appropriate for training. You can read more about this and check out the function that is used to perform this task [here](https://www.tensorflow.org/tutorials/structured_data/feature_columns). Then, we are going to map the training, validation, and test datasets using the function. Note that depending on the features that you use in the model, you may have to modify parts of the function."
      ],
      "metadata": {
        "id": "bNCpjs5GHnAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_dataset(dataframe, shuffle = True, batch_size = 128):\n",
        "    df = dataframe.copy()\n",
        "    labels = df.labels\n",
        "    df = df.text\n",
        "    ds = tf.data.Dataset.from_tensor_slices((df, labels))\n",
        "    if shuffle == True:\n",
        "        ds = ds.shuffle(buffer_size = len(df))\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return(ds)"
      ],
      "metadata": {
        "id": "UalPWlvleViL"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = df_to_dataset(train)\n",
        "val_ds = df_to_dataset(val)\n",
        "test_ds = df_to_dataset(test)"
      ],
      "metadata": {
        "id": "jYRIqtO4es4C"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifier Model using BERT"
      ],
      "metadata": {
        "id": "MTTBLbwuHxeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
      ],
      "metadata": {
        "id": "E7PqElg0QjI9"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "bert_encoder = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "metadata": {
        "id": "wMu9KRFNQv_b"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_input = tf.keras.layers.Input(shape = (), dtype = tf.string)\n",
        "encoder_input = bert_preprocess(text_input)\n",
        "encoder_output = bert_encoder(encoder_input)\n",
        "\n",
        "l = tf.keras.layers.Dense(100, activation = 'relu')(encoder_output['pooled_output'])\n",
        "l = tf.keras.layers.Dense(1, activation = 'sigmoid')(l)\n",
        "model = tf.keras.Model(inputs=[text_input], outputs = [l])"
      ],
      "metadata": {
        "id": "I9pyoNzuRoSx"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.005),\n",
        "              loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "SJbPZyzDReGA"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_ds,\n",
        "                    validation_data = val_ds,\n",
        "                    epochs = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRGHV0tUgPCd",
        "outputId": "73762992-945f-4218-829d-0b8f77ed5ad5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "16/16 [==============================] - 69s 4s/step - loss: 1.0159 - accuracy: 0.5390 - val_loss: 0.6686 - val_accuracy: 0.6528\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 64s 4s/step - loss: 0.6857 - accuracy: 0.5655 - val_loss: 0.6824 - val_accuracy: 0.6490\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 64s 4s/step - loss: 0.6793 - accuracy: 0.5755 - val_loss: 0.6714 - val_accuracy: 0.6186\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 64s 4s/step - loss: 0.6708 - accuracy: 0.5970 - val_loss: 0.6359 - val_accuracy: 0.6546\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 64s 4s/step - loss: 0.6675 - accuracy: 0.5925 - val_loss: 0.6529 - val_accuracy: 0.6205\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 64s 4s/step - loss: 0.6642 - accuracy: 0.5980 - val_loss: 0.6630 - val_accuracy: 0.6110\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 64s 4s/step - loss: 0.6498 - accuracy: 0.6245 - val_loss: 0.6128 - val_accuracy: 0.6812\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 64s 4s/step - loss: 0.6457 - accuracy: 0.6380 - val_loss: 0.7166 - val_accuracy: 0.4896\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 64s 4s/step - loss: 0.6483 - accuracy: 0.6195 - val_loss: 0.6187 - val_accuracy: 0.6603\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 64s 4s/step - loss: 0.6359 - accuracy: 0.6355 - val_loss: 0.6441 - val_accuracy: 0.6110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Model"
      ],
      "metadata": {
        "id": "mOr_A1eRIWys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(['he good person is a monster in the woods.'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uSZNBmcg0cb",
        "outputId": "f0082577-c042-4c37-de69-361daed02a8b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 248ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.36027667]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to test out the model using an example sentence that is grammatically wrong. As the prediction value is smaller than 0.5, the model predicted that the sentence is grammatically wrong. This time, we are going to evaluate the model using the test dataset:"
      ],
      "metadata": {
        "id": "uLuk9gKJIi2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiUftk-oIalO",
        "outputId": "3db12b10-0141-4667-b359-655478a5b8b9"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 14s 3s/step - loss: 0.6436 - accuracy: 0.6395\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6435754895210266, 0.6395348906517029]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model returned an accuracy of **63.95%**. The model ranks around 15th among the models that have been used to perform the identical task on [Kaggle](https://www.kaggle.com/competitions/cola-in-domain-open-evaluation/leaderboard) (although the leaderboard is quite outdated)."
      ],
      "metadata": {
        "id": "H_YLvZLbKyKB"
      }
    }
  ]
}