{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate --quiet","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:37:28.749718Z","iopub.execute_input":"2023-08-07T15:37:28.750426Z","iopub.status.idle":"2023-08-07T15:37:42.042052Z","shell.execute_reply.started":"2023-08-07T15:37:28.750388Z","shell.execute_reply":"2023-08-07T15:37:42.040698Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os, re, random, datasets, evaluate\n\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\npd.set_option('display.max_colwidth', None)\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-07T15:37:42.048970Z","iopub.execute_input":"2023-08-07T15:37:42.049924Z","iopub.status.idle":"2023-08-07T15:38:00.611554Z","shell.execute_reply.started":"2023-08-07T15:37:42.049884Z","shell.execute_reply":"2023-08-07T15:38:00.610431Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This notebook is an adapdation of a tutorial that I found [here](https://www.kaggle.com/code/rajkumarl/nlp-tutorial-fine-tuning-with-trainer-api/notebook). I've been looking for tutorials where we use fine-tuned models on Hugging Face for the purpose of classifying text using **transformers** on Youtube and Kaggle, and I found the one linked above really helpful. This serves as a way to try out and learn: **AutoTokenizer**, **AutoModelForSequenceClassification**, **DataCollatorWithPadding**, **TrainingArguments**, and **Trainer** among many others. ","metadata":{}},{"cell_type":"markdown","source":"# Import and Preprocess Data Set","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ntest_df = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:38:07.112168Z","iopub.execute_input":"2023-08-07T15:38:07.112927Z","iopub.status.idle":"2023-08-07T15:38:07.272378Z","shell.execute_reply.started":"2023-08-07T15:38:07.112888Z","shell.execute_reply":"2023-08-07T15:38:07.271220Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train['text'] = train.premise + \" [SEP] \" + train.hypothesis\ntest_df['text'] = test_df.premise + \" [SEP] \" + test_df.hypothesis","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:41:54.615930Z","iopub.execute_input":"2023-08-07T15:41:54.616355Z","iopub.status.idle":"2023-08-07T15:41:54.634787Z","shell.execute_reply.started":"2023-08-07T15:41:54.616315Z","shell.execute_reply":"2023-08-07T15:41:54.633719Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = np.split(train.sample(frac = 1), [int(0.8 * len(train))])","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:41:56.807867Z","iopub.execute_input":"2023-08-07T15:41:56.808248Z","iopub.status.idle":"2023-08-07T15:41:56.825156Z","shell.execute_reply.started":"2023-08-07T15:41:56.808215Z","shell.execute_reply":"2023-08-07T15:41:56.824038Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_dict = datasets.Dataset.from_dict(train_df.to_dict(orient=\"list\"))\nval_dict = datasets.Dataset.from_dict(val_df.to_dict(orient=\"list\"))\ntest_dict = datasets.Dataset.from_dict(test_df.to_dict(orient=\"list\"))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:42:01.032907Z","iopub.execute_input":"2023-08-07T15:42:01.033354Z","iopub.status.idle":"2023-08-07T15:42:01.188250Z","shell.execute_reply.started":"2023-08-07T15:42:01.033314Z","shell.execute_reply":"2023-08-07T15:42:01.187212Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"contradiction_ds = datasets.DatasetDict({\"train\": train_dict, \"val\": val_dict, \"test\": test_dict})","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:42:15.049495Z","iopub.execute_input":"2023-08-07T15:42:15.049904Z","iopub.status.idle":"2023-08-07T15:42:15.055178Z","shell.execute_reply.started":"2023-08-07T15:42:15.049873Z","shell.execute_reply":"2023-08-07T15:42:15.054058Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"contradiction_ds","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:42:16.541222Z","iopub.execute_input":"2023-08-07T15:42:16.541961Z","iopub.status.idle":"2023-08-07T15:42:16.549231Z","shell.execute_reply.started":"2023-08-07T15:42:16.541926Z","shell.execute_reply":"2023-08-07T15:42:16.548134Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'label', 'text'],\n        num_rows: 9696\n    })\n    val: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'label', 'text'],\n        num_rows: 2424\n    })\n    test: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'text'],\n        num_rows: 5195\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Pre-trained Model","metadata":{}},{"cell_type":"markdown","source":"We are going to find a model on Hugging Face that is best suited for the purpose of the classification task at hand. We hoped to find a model that (1) works on texts written in multiple languages; (2) is fine-tuned on Natural Language Inference (NLI) tasks. The model that meets the listed criteria and has been shown to perform moderately well for this task is [symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli](\"https://huggingface.co/symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli\"). ","metadata":{}},{"cell_type":"code","source":"model_name = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndata_collator = DataCollatorWithPadding(tokenizer = tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:52:13.732555Z","iopub.execute_input":"2023-08-07T15:52:13.732961Z","iopub.status.idle":"2023-08-07T15:52:15.589637Z","shell.execute_reply.started":"2023-08-07T15:52:13.732928Z","shell.execute_reply":"2023-08-07T15:52:15.588477Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/398 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67533345a78a419f8ad9613e23d17f78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e658497e7f2453ba9ce05f79ddd05a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7f31642bc314eda96ad1da7e60b8f08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e259ca01137b4404bbeabd429e8b9c6e"}},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 3)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:52:16.553877Z","iopub.execute_input":"2023-08-07T15:52:16.554375Z","iopub.status.idle":"2023-08-07T15:52:27.058619Z","shell.execute_reply.started":"2023-08-07T15:52:16.554331Z","shell.execute_reply":"2023-08-07T15:52:27.057520Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/921 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a3c2902973140c9aff09cb0cb3b611b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"760e282b2b094d9891aa4a418a3d13e3"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenize the Data Set","metadata":{}},{"cell_type":"code","source":"def tokenize_function(dataset):\n    return tokenizer(dataset['text'], truncation=True)\n\ntokenized_data = contradiction_ds.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:52:28.883138Z","iopub.execute_input":"2023-08-07T15:52:28.884004Z","iopub.status.idle":"2023-08-07T15:52:32.766186Z","shell.execute_reply.started":"2023-08-07T15:52:28.883969Z","shell.execute_reply":"2023-08-07T15:52:32.765148Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eebee9b82584568a18545ea9dd3e084"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"526ad8bf93fb4aee93f12d351baa77e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"558d1358887b4d83b5961c0611679a5a"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_data","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:48:04.664490Z","iopub.execute_input":"2023-08-07T15:48:04.664925Z","iopub.status.idle":"2023-08-07T15:48:04.671828Z","shell.execute_reply.started":"2023-08-07T15:48:04.664889Z","shell.execute_reply":"2023-08-07T15:48:04.670740Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'label', 'text', 'input_ids'],\n        num_rows: 9696\n    })\n    val: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'label', 'text', 'input_ids'],\n        num_rows: 2424\n    })\n    test: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'text', 'input_ids'],\n        num_rows: 5195\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_data = tokenized_data.remove_columns(['premise','hypothesis', 'lang_abv', 'language', 'text'])\ntokenized_data.with_format('pt')","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:52:35.192305Z","iopub.execute_input":"2023-08-07T15:52:35.192735Z","iopub.status.idle":"2023-08-07T15:52:35.210174Z","shell.execute_reply.started":"2023-08-07T15:52:35.192700Z","shell.execute_reply":"2023-08-07T15:52:35.209005Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 9696\n    })\n    val: Dataset({\n        features: ['id', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 2424\n    })\n    test: Dataset({\n        features: ['id', 'input_ids', 'attention_mask'],\n        num_rows: 5195\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Build and Train the Model","metadata":{}},{"cell_type":"markdown","source":"Before instantiating the trainer, we are going to create **TrainingArguments**. Following are the values we are going to set for each of the parameters:\n\n- model: **model_name** this is the pre-trained model we are going to use to train and evaluate the classifier;\n- evaluation_strategy: **'epoch'** determines when we are going to evaluate the performance of the classifier. We are going to do this at at the end of each epoch;\n- num_train_epochs: **5** is the number of training epochs;\n- learning_rate: **5e-5** is the learning rate;\n- weight_decay: **0.005** is the weight decay parameter (for regularization);\n- per_device_train_batch_size: **16** is the batch size per GPU/TPU core/CPU for training;\n- per_device_eval_batch_size: **16** is the batch size per GPU/TPU core/CPU for evaluation;\n- report_to: **'none'** is the integrations where the results and logs are reported to. We are not integrating. ","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(model_name,  \n                                  evaluation_strategy = 'epoch',\n                                  num_train_epochs = 5,\n                                  learning_rate = 5e-5,\n                                  weight_decay = 0.005,\n                                  per_device_train_batch_size = 16,\n                                  per_device_eval_batch_size = 16,\n                                  report_to = 'none')","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:52:37.842212Z","iopub.execute_input":"2023-08-07T15:52:37.843344Z","iopub.status.idle":"2023-08-07T15:52:37.850326Z","shell.execute_reply.started":"2023-08-07T15:52:37.843296Z","shell.execute_reply":"2023-08-07T15:52:37.849127Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"I am told that when using trainer, we have to define a function that computes the metric(s) for us (as opposed to simply saying **metrics == ['accuracy']**. This is one example of such function: ","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    metric = evaluate.load(\"accuracy\")\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis = -1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:52:40.417826Z","iopub.execute_input":"2023-08-07T15:52:40.418201Z","iopub.status.idle":"2023-08-07T15:52:40.424064Z","shell.execute_reply.started":"2023-08-07T15:52:40.418168Z","shell.execute_reply":"2023-08-07T15:52:40.422972Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Now, we are going to instantiate a Trainer: \n\n- model: **model_name** this is the pre-trained model we are going to use to train and evaluate the classifier;\n- args: **training_args** are the arguments used for training. We set these above; \n- train_dataset: **tokenized_data[\"train\"]** is the training dataset; \n- eval_dataset: **tokenized_data[\"val\"]** is the validation dataset;\n- data_collator: **data_collator** is a function that we use to form batches. DataCollatorWithPadding is used as we provide tokenizer;\n- tokenizer: **tokenizer** is the tokenizer that is used to preprocess the data. This automatically pads the inputs to maximum length; \n- compute_metris: **compute_metrics** is the function that computes the metric(s) at evaluation. We defined this funciton above.","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    training_args,\n    train_dataset = tokenized_data[\"train\"],\n    eval_dataset = tokenized_data[\"val\"],\n    data_collator = data_collator,\n    tokenizer = tokenizer,\n    compute_metrics = compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:52:42.655629Z","iopub.execute_input":"2023-08-07T15:52:42.656123Z","iopub.status.idle":"2023-08-07T15:52:43.132477Z","shell.execute_reply.started":"2023-08-07T15:52:42.656081Z","shell.execute_reply":"2023-08-07T15:52:43.131285Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:52:45.941764Z","iopub.execute_input":"2023-08-07T15:52:45.942250Z","iopub.status.idle":"2023-08-07T16:10:17.154412Z","shell.execute_reply.started":"2023-08-07T15:52:45.942205Z","shell.execute_reply":"2023-08-07T16:10:17.153266Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1515' max='1515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1515/1515 17:29, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.361899</td>\n      <td>0.857261</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.361900</td>\n      <td>0.389425</td>\n      <td>0.853135</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.361900</td>\n      <td>0.512928</td>\n      <td>0.869637</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.163000</td>\n      <td>0.653243</td>\n      <td>0.873762</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.061100</td>\n      <td>0.763527</td>\n      <td>0.875413</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3cfd98f057f406eb29c992644faa66a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1515, training_loss=0.1939331634603318, metrics={'train_runtime': 1049.9833, 'train_samples_per_second': 46.172, 'train_steps_per_second': 1.443, 'total_flos': 2427221278540800.0, 'train_loss': 0.1939331634603318, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Prepare for Submission","metadata":{}},{"cell_type":"markdown","source":"Finally, we are going to use the trainer to predict the label for the texts inside the test data set. ","metadata":{}},{"cell_type":"code","source":"test_predictions = trainer.predict(tokenized_data[\"test\"])\npreds = np.argmax(test_predictions.predictions, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T16:10:17.156671Z","iopub.execute_input":"2023-08-07T16:10:17.157036Z","iopub.status.idle":"2023-08-07T16:10:49.744755Z","shell.execute_reply.started":"2023-08-07T16:10:17.157001Z","shell.execute_reply":"2023-08-07T16:10:49.743669Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"submission = pd.DataFrame(list(zip(test_df.id, preds)), \n                          columns = [\"id\", \"prediction\"])\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T16:10:49.746541Z","iopub.execute_input":"2023-08-07T16:10:49.746903Z","iopub.status.idle":"2023-08-07T16:10:49.808425Z","shell.execute_reply.started":"2023-08-07T16:10:49.746867Z","shell.execute_reply":"2023-08-07T16:10:49.807422Z"},"trusted":true},"execution_count":39,"outputs":[]}]}