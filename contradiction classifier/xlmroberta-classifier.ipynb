{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:28:58.469275Z","iopub.execute_input":"2023-08-07T07:28:58.469698Z","iopub.status.idle":"2023-08-07T07:29:10.719696Z","shell.execute_reply.started":"2023-08-07T07:28:58.469650Z","shell.execute_reply":"2023-08-07T07:29:10.718614Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.0)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os, re, random, datasets, evaluate\n\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\npd.set_option('display.max_colwidth', None)\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-07T07:29:10.724195Z","iopub.execute_input":"2023-08-07T07:29:10.724519Z","iopub.status.idle":"2023-08-07T07:29:22.162563Z","shell.execute_reply.started":"2023-08-07T07:29:10.724490Z","shell.execute_reply":"2023-08-07T07:29:22.161577Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"This notebook is an adapdation of a tutorial that I found [here](https://www.kaggle.com/code/rajkumarl/nlp-tutorial-fine-tuning-with-trainer-api/notebook). I've been looking for tutorials where we fine-tune BERT (and BERT-equivalent) for the purpose of classifying text using **transformers** on Youtube and Kaggle, and I found the one linked above really helpful. This serves as a way to try out and learn: **AutoTokenizer**, **AutoModelForSequenceClassification**, **DataCollatorWithPadding**, **TrainingArguments**, and **Trainer**.","metadata":{}},{"cell_type":"markdown","source":"## Import and Preprocess Data Set","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ntest_df = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:22.163997Z","iopub.execute_input":"2023-08-07T07:29:22.164342Z","iopub.status.idle":"2023-08-07T07:29:22.387262Z","shell.execute_reply.started":"2023-08-07T07:29:22.164310Z","shell.execute_reply":"2023-08-07T07:29:22.386137Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train['text'] = train.hypothesis + \" [SEP] \" + train.premise\ntest_df['text'] = test_df.hypothesis + \" [SEP] \" + test_df.premise","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:22.389942Z","iopub.execute_input":"2023-08-07T07:29:22.390317Z","iopub.status.idle":"2023-08-07T07:29:22.414064Z","shell.execute_reply.started":"2023-08-07T07:29:22.390283Z","shell.execute_reply":"2023-08-07T07:29:22.413101Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = np.split(train.sample(frac = 1), [int(0.8 * len(train))])","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:22.415345Z","iopub.execute_input":"2023-08-07T07:29:22.416012Z","iopub.status.idle":"2023-08-07T07:29:22.436468Z","shell.execute_reply.started":"2023-08-07T07:29:22.415980Z","shell.execute_reply":"2023-08-07T07:29:22.435325Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dict = datasets.Dataset.from_dict(train_df.to_dict(orient=\"list\"))\nval_dict = datasets.Dataset.from_dict(val_df.to_dict(orient=\"list\"))\ntest_dict = datasets.Dataset.from_dict(test_df.to_dict(orient=\"list\"))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:22.438235Z","iopub.execute_input":"2023-08-07T07:29:22.439369Z","iopub.status.idle":"2023-08-07T07:29:22.583041Z","shell.execute_reply.started":"2023-08-07T07:29:22.439337Z","shell.execute_reply":"2023-08-07T07:29:22.582078Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"contradiction_ds = datasets.DatasetDict({\"train\": train_dict, \"val\": val_dict, \"test\": test_dict})","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:22.584460Z","iopub.execute_input":"2023-08-07T07:29:22.584805Z","iopub.status.idle":"2023-08-07T07:29:22.589891Z","shell.execute_reply.started":"2023-08-07T07:29:22.584772Z","shell.execute_reply":"2023-08-07T07:29:22.588843Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"contradiction_ds","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:22.591355Z","iopub.execute_input":"2023-08-07T07:29:22.591986Z","iopub.status.idle":"2023-08-07T07:29:22.607591Z","shell.execute_reply.started":"2023-08-07T07:29:22.591952Z","shell.execute_reply":"2023-08-07T07:29:22.606574Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'label', 'text'],\n        num_rows: 9696\n    })\n    val: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'label', 'text'],\n        num_rows: 2424\n    })\n    test: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'text'],\n        num_rows: 5195\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"model_name = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndata_collator = DataCollatorWithPadding(tokenizer = tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:22.609281Z","iopub.execute_input":"2023-08-07T07:29:22.609658Z","iopub.status.idle":"2023-08-07T07:29:24.520137Z","shell.execute_reply.started":"2023-08-07T07:29:22.609612Z","shell.execute_reply":"2023-08-07T07:29:24.519128Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/398 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51bf53f7b1994c9a93f4104d7334bc25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47216c38f32445d0aea6b41c8696f7f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c9ad87847f74d3ea1892c1e6165883a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43b9dee70720497f845fab850bf731b3"}},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 3)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:24.524175Z","iopub.execute_input":"2023-08-07T07:29:24.524488Z","iopub.status.idle":"2023-08-07T07:29:43.657469Z","shell.execute_reply.started":"2023-08-07T07:29:24.524462Z","shell.execute_reply":"2023-08-07T07:29:43.656405Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/921 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0a30163435a40108863015f90b3b0d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe6259ea07174418a222246989250ca3"}},"metadata":{}}]},{"cell_type":"code","source":"def Tokenize_function(example):\n    return tokenizer(example['text'], truncation=True)\n\ntokenized_data = contradiction_ds.map(Tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:43.659058Z","iopub.execute_input":"2023-08-07T07:29:43.659576Z","iopub.status.idle":"2023-08-07T07:29:47.370548Z","shell.execute_reply.started":"2023-08-07T07:29:43.659536Z","shell.execute_reply":"2023-08-07T07:29:47.369604Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cd7a86e2141491ebd6221f61aebfad2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e50346fd276475db56cccc9e0304ef6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f097421de7ba4373b1e20f5514656331"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_data","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:47.372082Z","iopub.execute_input":"2023-08-07T07:29:47.372462Z","iopub.status.idle":"2023-08-07T07:29:47.378673Z","shell.execute_reply.started":"2023-08-07T07:29:47.372426Z","shell.execute_reply":"2023-08-07T07:29:47.377803Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'label', 'text', 'input_ids', 'attention_mask'],\n        num_rows: 9696\n    })\n    val: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'label', 'text', 'input_ids', 'attention_mask'],\n        num_rows: 2424\n    })\n    test: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'text', 'input_ids', 'attention_mask'],\n        num_rows: 5195\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_data = tokenized_data.remove_columns(['premise','hypothesis', 'lang_abv', 'language', 'text'])\ntokenized_data.with_format('pt')","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:47.382783Z","iopub.execute_input":"2023-08-07T07:29:47.383428Z","iopub.status.idle":"2023-08-07T07:29:47.536607Z","shell.execute_reply.started":"2023-08-07T07:29:47.383394Z","shell.execute_reply":"2023-08-07T07:29:47.535645Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 9696\n    })\n    val: Dataset({\n        features: ['id', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 2424\n    })\n    test: Dataset({\n        features: ['id', 'input_ids', 'attention_mask'],\n        num_rows: 5195\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"training_args = TrainingArguments(model_name,  \n                                  evaluation_strategy='epoch',\n                                  num_train_epochs = 5,\n                                  learning_rate = 5e-5,\n                                  weight_decay = 0.005,\n                                  per_device_train_batch_size = 16,\n                                  per_device_eval_batch_size = 16,\n                                  report_to = 'none')","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:47.538149Z","iopub.execute_input":"2023-08-07T07:29:47.538577Z","iopub.status.idle":"2023-08-07T07:29:47.630909Z","shell.execute_reply.started":"2023-08-07T07:29:47.538544Z","shell.execute_reply":"2023-08-07T07:29:47.629896Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    metric = evaluate.load(\"accuracy\")\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:47.633562Z","iopub.execute_input":"2023-08-07T07:29:47.634256Z","iopub.status.idle":"2023-08-07T07:29:47.639840Z","shell.execute_reply.started":"2023-08-07T07:29:47.634220Z","shell.execute_reply":"2023-08-07T07:29:47.638830Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    training_args,\n    train_dataset = tokenized_data[\"train\"],\n    eval_dataset = tokenized_data[\"val\"],\n    data_collator = data_collator,\n    tokenizer = tokenizer,\n    compute_metrics = compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:47.640922Z","iopub.execute_input":"2023-08-07T07:29:47.641173Z","iopub.status.idle":"2023-08-07T07:29:56.146151Z","shell.execute_reply.started":"2023-08-07T07:29:47.641151Z","shell.execute_reply":"2023-08-07T07:29:56.144918Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:29:56.147529Z","iopub.execute_input":"2023-08-07T07:29:56.148487Z","iopub.status.idle":"2023-08-07T07:46:27.651601Z","shell.execute_reply.started":"2023-08-07T07:29:56.148449Z","shell.execute_reply":"2023-08-07T07:46:27.650579Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1515' max='1515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1515/1515 16:23, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.429494</td>\n      <td>0.832096</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.508200</td>\n      <td>0.452934</td>\n      <td>0.831271</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.508200</td>\n      <td>0.509621</td>\n      <td>0.842409</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.207800</td>\n      <td>0.721203</td>\n      <td>0.846947</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.080700</td>\n      <td>0.897081</td>\n      <td>0.842409</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5128f9a4af92472d84749931e196d346"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1515, training_loss=0.2632287571729213, metrics={'train_runtime': 991.1404, 'train_samples_per_second': 48.913, 'train_steps_per_second': 1.529, 'total_flos': 2424540810597696.0, 'train_loss': 0.2632287571729213, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Prepare for Submission","metadata":{}},{"cell_type":"code","source":"yhat = trainer.predict(tokenized_data[\"test\"])\npreds = np.argmax(yhat.predictions, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:46:27.653549Z","iopub.execute_input":"2023-08-07T07:46:27.654711Z","iopub.status.idle":"2023-08-07T07:46:58.090939Z","shell.execute_reply.started":"2023-08-07T07:46:27.654670Z","shell.execute_reply":"2023-08-07T07:46:58.089978Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"submission = pd.DataFrame(list(zip(test_df.id, preds)), \n                          columns = [\"id\", \"prediction\"])\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:49:30.492539Z","iopub.execute_input":"2023-08-07T07:49:30.493271Z","iopub.status.idle":"2023-08-07T07:49:30.550408Z","shell.execute_reply.started":"2023-08-07T07:49:30.493236Z","shell.execute_reply":"2023-08-07T07:49:30.549398Z"},"trusted":true},"execution_count":26,"outputs":[]}]}